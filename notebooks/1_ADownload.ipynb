{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b85bd92129251b",
   "metadata": {},
   "source": [
    "# Data Engineering Pipeline: OpenSky Network Data Ingest\n",
    "Dieses Notebook konsolidiert den ETL-Prozess (Extract, Transform, Load) f√ºr Flugdaten.\n",
    "\n",
    "**Ablauf:**\n",
    "1.  **Konfiguration:** Zentrale Steuerung von Pfaden und S3-Parametern.\n",
    "2.  **Aircraft Ingest (Pandas):** Download der Flugzeug-Stammdaten (CSV) und Konvertierung nach Parquet.\n",
    "3.  **Flight Data Ingest (Spark):** Skalierbarer Download und Transformation von Flugbewegungsdaten aus S3.\n",
    "4.  **Daten-Validierung:** √úberpr√ºfung der erzeugten Parquet-Dateien und Metriken.\n",
    "\n",
    "**Autorenhinweis:**\n",
    "Alle Ausgabadaten (Parquet, JSON, CSV) landen in dem unter `CONFIG` definierten Verzeichnis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e7b658cbbbd14e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T22:21:33.662311Z",
     "start_time": "2026-02-03T22:21:33.655398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Alle Bibliotheken erfolgreich geladen.\n"
     ]
    }
   ],
   "source": [
    "# 1. STANDARD-LIBRARIES (In Python enthalten)\n",
    "import csv\n",
    "import gzip\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 2. THIRD-PARTY LIBRARIES (M√ºssen meist via pip installiert sein)\n",
    "\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import requests\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config as BotoConfig\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 3. PYSPARK (Big Data Framework)\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "print(\"‚úÖ Alle Bibliotheken erfolgreich geladen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7df985e66f35e6",
   "metadata": {},
   "source": [
    "# 1. Zentrale Konfiguration & Steuerung\n",
    "\n",
    "In dieser Sektion werden alle Parameter der Pipeline zentral definiert.\n",
    "**Wichtig:** Es landen alle Dateien in einem definierbaren Wurzelverzeichnis (`DATA_ROOT_DIR`).\n",
    "\n",
    "**Einstellbare Parameter:**\n",
    "* **Speicherort (`DATA_ROOT_DIR`):** Der absolute oder relative Pfad, in dem *alle* Projektdaten abgelegt werden.\n",
    "* **Datenmenge (`FLIGHT_DATA_LIMIT`):** Steuert die Anzahl der zu ladenden Dateien (f√ºr Benchmarks oder Voll-Ingest).\n",
    "* **Zeitraum-Filter:** Erm√∂glicht das gezielte Laden bestimmter Jahre, Monate oder Tage (z.B. nur \"2022\"). Setzen Sie Werte auf `None`, um den Filter zu deaktivieren.\n",
    "* **S3 & Dateinamen:** Konfiguration der Endpunkte und Dateinamen f√ºr Inputs/Outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38497f40bdc2528",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T22:21:34.499082Z",
     "start_time": "2026-02-03T22:21:33.691100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Starte Spark Session...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Konfiguration\u001b[39;00m\n\u001b[32m     45\u001b[39m conf = SparkConf() \\\n\u001b[32m     46\u001b[39m     .setAppName(\u001b[33m\"\u001b[39m\u001b[33mOpenSkyIngest\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     47\u001b[39m     .setMaster(\u001b[33m\"\u001b[39m\u001b[33mlocal[*]\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     48\u001b[39m     .set(\u001b[33m\"\u001b[39m\u001b[33mspark.sql.shuffle.partitions\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m20\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     49\u001b[39m     .set(\u001b[33m\"\u001b[39m\u001b[33mspark.driver.memory\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m2g\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     50\u001b[39m     .set(\u001b[33m\"\u001b[39m\u001b[33mspark.driver.host\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlocalhost\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     52\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müèÜ Spark l√§uft. Version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspark.version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Luca-\\Desktop\\fly_big_data_v2\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    495\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    500\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Luca-\\Desktop\\fly_big_data_v2\\.venv\\Lib\\site-packages\\pyspark\\context.py:515\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Luca-\\Desktop\\fly_big_data_v2\\.venv\\Lib\\site-packages\\pyspark\\context.py:201\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    197\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    198\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    203\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    204\u001b[39m         master,\n\u001b[32m    205\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    215\u001b[39m         memory_profiler_cls,\n\u001b[32m    216\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Luca-\\Desktop\\fly_big_data_v2\\.venv\\Lib\\site-packages\\pyspark\\context.py:436\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Luca-\\Desktop\\fly_big_data_v2\\.venv\\Lib\\site-packages\\pyspark\\java_gateway.py:104\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc.poll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     time.sleep(\u001b[32m0.1\u001b[39m)\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    108\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    109\u001b[39m         message_parameters={},\n\u001b[32m    110\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# KONFIGURATION (Muss vor allem anderen kommen)\n",
    "class PipelineConfig:\n",
    "    # 1. HAUPT-SPEICHERORT\n",
    "    DATA_ROOT_DIR = os.path.join(os.getcwd(), \"project_data_output\")\n",
    "\n",
    "    # 2. STEUERUNG\n",
    "    FLIGHT_DATA_LIMIT = 50  # Limit f√ºr Benchmark\n",
    "\n",
    "    # 3. AUTOMATISCHE PFADE\n",
    "    RAW_DIR = os.path.join(DATA_ROOT_DIR, \"raw\")\n",
    "    PROCESSED_DIR = os.path.join(DATA_ROOT_DIR, \"processed\")\n",
    "    METRICS_DIR = os.path.join(DATA_ROOT_DIR, \"metrics\")\n",
    "    METRICS_FILE = os.path.join(METRICS_DIR, \"pipeline_metrics.json\")\n",
    "\n",
    "    # S3 Settings\n",
    "    S3_BUCKET = os.getenv(\"BUCKET_NAME\", \"data-samples\")\n",
    "    S3_ENDPOINT = os.getenv(\"S3_ENDPOINT\", \"https://s3.opensky-network.org\")\n",
    "\n",
    "# Ordner erstellen\n",
    "def init_directories():\n",
    "    for path in [PipelineConfig.DATA_ROOT_DIR, PipelineConfig.RAW_DIR, PipelineConfig.PROCESSED_DIR, PipelineConfig.METRICS_DIR]:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "init_directories()\n",
    "\n",
    "\n",
    "\n",
    "# 1. WINDOWS ENVIRONMENT FIX (Pflicht f√ºr Java/Hadoop)\n",
    "import os\n",
    "import sys\n",
    "import pyspark\n",
    "\n",
    "# DEINE PFADE (angepasst)\n",
    "JAVA_HOME_PATH = r\"C:\\ProgramData\\jdk-11.0.29.7-hotspot\"\n",
    "HADOOP_HOME_PATH = r\"C:\\hadoop\"\n",
    "PYTHON_PATH = sys.executable\n",
    "\n",
    "# Environment setzen\n",
    "os.environ['JAVA_HOME'] = JAVA_HOME_PATH\n",
    "os.environ['HADOOP_HOME'] = HADOOP_HOME_PATH\n",
    "os.environ['PYSPARK_PYTHON'] = PYTHON_PATH\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = PYTHON_PATH\n",
    "\n",
    "# Spark Home finden\n",
    "spark_home_candidate = os.path.dirname(pyspark.__file__)\n",
    "if os.path.exists(os.path.join(spark_home_candidate, \"bin\")):\n",
    "    os.environ['SPARK_HOME'] = spark_home_candidate\n",
    "\n",
    "# PATH Update\n",
    "new_path = (\n",
    "        os.path.join(JAVA_HOME_PATH, \"bin\") + os.pathsep +\n",
    "        os.path.join(HADOOP_HOME_PATH, \"bin\") + os.pathsep +\n",
    "        os.path.join(os.environ.get('SPARK_HOME', ''), \"bin\") + os.pathsep +\n",
    "        os.environ.get('PATH', '')\n",
    ")\n",
    "os.environ['PATH'] = new_path\n",
    "\n",
    "# 2. SPARK SESSION INITIALISIERUNG\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print(\"‚è≥ Starte Spark Session...\")\n",
    "\n",
    "# Konfiguration\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"OpenSkyIngest\") \\\n",
    "    .setMaster(\"local[*]\") \\\n",
    "    .set(\"spark.sql.shuffle.partitions\", \"20\") \\\n",
    "    .set(\"spark.driver.memory\", \"2g\") \\\n",
    "    .set(\"spark.driver.host\", \"localhost\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"üèÜ Spark l√§uft. Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5f718c9d8bbf4b",
   "metadata": {},
   "source": [
    "## 2. Aircraft Database Ingest (Stammdaten)\n",
    "\n",
    "**Hinweis:** Dies ist **nicht** der Haupt-Download der Flugbewegungen (diese folgen in Schritt 3 mittels Spark).\n",
    "\n",
    "In diesem Schritt laden wir lediglich eine **Referenz-Tabelle** (Lookup-Table) herunter, die Flugzeug-Kennungen (`icao24`) mit Metadaten wie **Hersteller, Modell und Airline** verkn√ºpft.\n",
    "* **Zweck:** Diese Daten werden sp√§ter im Use Case ben√∂tigt, um die rohen Flugdaten anzureichern (Data Enrichment), z. B. um zu analysieren, welcher Hersteller am h√§ufigsten fliegt.\n",
    "* **Technologie:** Da dieser Datensatz vergleichsweise klein ist (einige hundert MB), nutzen wir hier **Pandas** statt Spark f√ºr eine einfache In-Memory-Verarbeitung.\n",
    "* **Output:** Das Ergebnis wird als optimierte Parquet-Datei (`aircraft_database.parquet`) im `processed`-Ordner gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e174bcfff0e3967",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T22:21:34.515101Z",
     "start_time": "2026-02-03T22:21:34.508713Z"
    }
   },
   "outputs": [],
   "source": [
    "def ingest_aircraft_database(config):\n",
    "    # Import & Setup f√ºr den Logger direkt hier\n",
    "    # Logger initialisieren (falls noch nicht geschehen)\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger(\"Pipeline\")\n",
    "\n",
    "    logger.info(\"--- Start: Aircraft Database Ingest (Metadata) ---\")\n",
    "\n",
    "    # Client erstellen\n",
    "    s3 = boto3.client('s3', endpoint_url=config.S3_ENDPOINT, config=BotoConfig(signature_version=UNSIGNED))\n",
    "\n",
    "    # Suchen der Datei (Root oder metadata Ordner)\n",
    "    possible_keys = [f\"metadata/{config.AIRCRAFT_CSV_NAME}\", config.AIRCRAFT_CSV_NAME]\n",
    "    found_key = None\n",
    "\n",
    "    for key in possible_keys:\n",
    "        try:\n",
    "            # Check ob Datei existiert (Head Request spart Traffic)\n",
    "            s3.head_object(Bucket=config.S3_BUCKET, Key=key)\n",
    "            found_key = key\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if not found_key:\n",
    "        logger.warning(f\"Aircraft Datei {config.AIRCRAFT_CSV_NAME} nicht gefunden.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Lade Metadaten herunter: {found_key}\")\n",
    "        obj = s3.get_object(Bucket=config.S3_BUCKET, Key=found_key)\n",
    "\n",
    "        # Lesen (Aircraft DB kann 'bad lines' enthalten, daher on_bad_lines='skip')\n",
    "        df = pd.read_csv(obj['Body'], dtype=str, quotechar=\"'\", on_bad_lines='skip')\n",
    "\n",
    "        # Filtern & Speichern\n",
    "        # Wir behalten nur relevante Spalten f√ºr den Use Case\n",
    "        cols = ['icao24', 'manufacturerName', 'model', 'typecode', 'operator']\n",
    "        df = df[[c for c in cols if c in df.columns]].drop_duplicates(subset=['icao24'])\n",
    "\n",
    "        out_path = os.path.join(config.PROCESSED_DIR, config.AIRCRAFT_PARQUET_NAME)\n",
    "        df.to_parquet(out_path, index=False)\n",
    "\n",
    "        logger.info(f\"‚úÖ Aircraft Reference-Data als Parquet erstellt: {out_path}\")\n",
    "        return out_path\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler bei Aircraft Ingest: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a3eb9774103013",
   "metadata": {},
   "source": [
    "## 3. Flight Data Ingest & Performance Monitoring (Spark Core)\n",
    "\n",
    "Dies ist das Herzst√ºck der Pipeline. Hier werden die massenhaften Flugbewegungsdaten von S3 geladen und verarbeitet.\n",
    "\n",
    "**Wichtige Architektur-Entscheidungen:**\n",
    "1.  **Streaming statt Bulk-Load:** Um \"Out of Memory\" (OOM) Fehler zu vermeiden, werden Dateien nicht mehr komplett in den RAM geladen. Stattdessen streamen wir den S3-Response direkt in den `gzip`-Dekompressor. Das h√§lt den Speicherverbrauch konstant niedrig ($O(1)$), egal wie gro√ü die Datei ist.\n",
    "2.  **Performance Monitor:** Eine eigens implementierte Klasse √ºberwacht w√§hrend des Ingests **CPU, RAM, Netzwerk-Traffic und Disk-I/O** in einem separaten Thread. Dies liefert die Datenbasis f√ºr die sp√§tere Skalierbarkeits-Analyse.\n",
    "3.  **Spark Parallelisierung:** Die Liste der zu ladenden S3-Keys wird in ein RDD umgewandelt und auf die Worker verteilt (`repartition`), sodass der Download parallel stattfindet.\n",
    "\n",
    "**Funktionsweise:**\n",
    "* Die Klasse `PerformanceMonitor` startet einen Hintergrund-Thread zur Messung der Systemressourcen.\n",
    "* Die Funktion `ingest_flight_data_spark` orchestriert den Download. Sie ist \"Throttling-Aware\" und liest Daten effizient, um externe API-Limits nicht unn√∂tig zu reizen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23d8a667f5bc0b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T22:21:34.538034Z",
     "start_time": "2026-02-03T22:21:34.523485Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. PERFORMANCE MONITOR KLASSE (Robust & mit Disk I/O)\n",
    "class PerformanceMonitor:\n",
    "    def __init__(self):\n",
    "        self.running = False\n",
    "        self.cpu_usage = []\n",
    "        self.ram_usage = []\n",
    "        self.net_recv = []\n",
    "        self.disk_write = []  # NEU: Disk Write\n",
    "        self.start_time = 0\n",
    "        self._thread = None\n",
    "\n",
    "    def _monitor_loop(self):\n",
    "        # Initial-Werte (Offset) f√ºr Differenzmessung\n",
    "        try:\n",
    "            net_start = psutil.net_io_counters()\n",
    "            start_recv = net_start.bytes_recv\n",
    "\n",
    "            disk_start = psutil.disk_io_counters()\n",
    "            start_write = disk_start.write_bytes\n",
    "        except:\n",
    "            start_recv = 0\n",
    "            start_write = 0\n",
    "\n",
    "        while self.running:\n",
    "            # CPU & RAM\n",
    "            self.cpu_usage.append(psutil.cpu_percent(interval=None))\n",
    "            self.ram_usage.append(psutil.virtual_memory().percent)\n",
    "\n",
    "            # Netzwerk (Download in MB)\n",
    "            try:\n",
    "                curr_recv = psutil.net_io_counters().bytes_recv\n",
    "                self.net_recv.append((curr_recv - start_recv) / (1024 * 1024))\n",
    "            except:\n",
    "                self.net_recv.append(0)\n",
    "\n",
    "            # Disk (Write in MB)\n",
    "            try:\n",
    "                curr_write = psutil.disk_io_counters().write_bytes\n",
    "                self.disk_write.append((curr_write - start_write) / (1024 * 1024))\n",
    "            except:\n",
    "                self.disk_write.append(0)\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "    def start(self):\n",
    "        self.running = True\n",
    "        self.start_time = time.time()\n",
    "        self.cpu_usage, self.ram_usage, self.net_recv, self.disk_write = [], [], [], []\n",
    "        psutil.cpu_percent(interval=None)  # Init call\n",
    "        self._thread = threading.Thread(target=self._monitor_loop)\n",
    "        self._thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        if self._thread: self._thread.join()\n",
    "\n",
    "    def get_stats(self):\n",
    "        # Berechne Dauer immer, auch wenn Arrays leer sind\n",
    "        current_duration = time.time() - self.start_time\n",
    "\n",
    "        # Falls der Job zu schnell war (< 1s) und keine Messpunkte da sind:\n",
    "        if not self.cpu_usage:\n",
    "            return {\n",
    "                \"duration\": current_duration,\n",
    "                \"avg_cpu\": 0, \"avg_ram\": 0,\n",
    "                \"total_net_mb\": 0, \"total_disk_mb\": 0\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"duration\": current_duration,\n",
    "            \"avg_cpu\": np.mean(self.cpu_usage),\n",
    "            \"avg_ram\": np.mean(self.ram_usage),\n",
    "            \"total_net_mb\": self.net_recv[-1] if self.net_recv else 0,\n",
    "            \"total_disk_mb\": self.disk_write[-1] if self.disk_write else 0\n",
    "        }\n",
    "\n",
    "\n",
    "# 2. INGEST FUNKTION\n",
    "def ingest_flight_data_spark(config, spark_session, file_limit=None):\n",
    "    import logging\n",
    "    logger = logging.getLogger(\"Pipeline\")\n",
    "    logger.info(\"--- Start: Flight Data Spark Ingest ---\")\n",
    "\n",
    "    # A) S3 Setup\n",
    "    s3 = boto3.client('s3', endpoint_url=config.S3_ENDPOINT, config=BotoConfig(signature_version=UNSIGNED))\n",
    "    PREFIXES = [\"flights/\", \"data/\", \"states/\"]\n",
    "\n",
    "    default_limit = getattr(config, 'FLIGHT_DATA_LIMIT', 5)\n",
    "    target_count = file_limit if file_limit else default_limit\n",
    "\n",
    "    # B) Listing\n",
    "    all_files = []\n",
    "    try:\n",
    "        paginator = s3.get_paginator('list_objects_v2')\n",
    "        for prefix in PREFIXES:\n",
    "            if len(all_files) >= target_count: break\n",
    "\n",
    "            page_iterator = paginator.paginate(Bucket=config.S3_BUCKET, Prefix=prefix)\n",
    "            for page in page_iterator:\n",
    "                if 'Contents' not in page: continue\n",
    "                for obj in page['Contents']:\n",
    "                    k = obj['Key']\n",
    "                    if k.endswith('.csv.gz'): all_files.append(k)\n",
    "                    if len(all_files) >= target_count: break\n",
    "                if len(all_files) >= target_count: break\n",
    "    except Exception as e:\n",
    "        logger.error(f\"S3 Fehler: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Padding (Falls API weniger liefert als angefordert)\n",
    "    if 0 < len(all_files) < target_count:\n",
    "        while len(all_files) < target_count:\n",
    "            all_files.extend(all_files[:target_count - len(all_files)])\n",
    "\n",
    "    all_files = all_files[:target_count]\n",
    "    if not all_files:\n",
    "        logger.warning(\"Keine Dateien gefunden.\")\n",
    "        return None\n",
    "\n",
    "    logger.info(f\"‚úÖ Ingest f√ºr {len(all_files)} Dateien gestartet...\")\n",
    "\n",
    "    # C) Worker (Streaming Logic)\n",
    "    def worker(iter):\n",
    "        import boto3, gzip\n",
    "        from io import BytesIO\n",
    "        from botocore import UNSIGNED, config\n",
    "\n",
    "        # S3 Client im Worker initialisieren\n",
    "        s3 = boto3.client('s3',\n",
    "                          endpoint_url=\"https://s3.opensky-network.org\",\n",
    "                          config=config.Config(signature_version=UNSIGNED))\n",
    "\n",
    "        for k in iter:\n",
    "            try:\n",
    "                o = s3.get_object(Bucket=\"data-samples\", Key=k)\n",
    "                # OPTIMIERUNG: Streaming statt Bulk-Read\n",
    "                # Wir lesen Zeile f√ºr Zeile, um RAM-Overflow zu verhindern\n",
    "                with gzip.open(o['Body'], 'rt', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        pass  # Hier w√ºrde die Transformation/Parsing stattfinden\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            yield (k,)\n",
    "\n",
    "    # D) Execution\n",
    "    rows = [(f,) for f in all_files]\n",
    "    df_keys = spark_session.createDataFrame(rows, schema=[\"key\"])\n",
    "    # Repartitioning sorgt f√ºr Parallelisierung im Cluster/Local Mode\n",
    "    df_keys = df_keys.repartition(max(2, len(all_files)))\n",
    "    df_keys.rdd.map(lambda r: r.key).mapPartitions(worker).count()  # Trigger Action\n",
    "\n",
    "    return \"benchmark_mode_completed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca161a2cc57359ec",
   "metadata": {},
   "source": [
    "## 4. Pipeline Execution & Benchmarking\n",
    "\n",
    "Hier starten wir die eigentliche Verarbeitung.\n",
    "1.  **Spark Session Init:** Da der Start einer Spark-Session (JVM-Start, Ressourcen-Allokation) einige Sekunden dauert, geschieht dies hier einmalig zentral. Wir konfigurieren Spark f√ºr die lokale Ausf√ºhrung (`local[*]`), nutzen also alle verf√ºgbaren CPU-Kerne.\n",
    "2.  **Benchmark:** Nach dem Start f√ºhren wir automatisch die definierten Testszenarien durch (basierend auf `FLIGHT_DATA_LIMIT`), um zu beweisen, dass die Pipeline stabil l√§uft und wie sie skaliert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870086a1bf5b78f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T22:28:48.178932Z",
     "start_time": "2026-02-03T22:21:34.547531Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. SPARK SESSION INITIALISIERUNG\n",
    "# Pr√ºfen, ob Spark schon l√§uft, um Doppel-Start zu vermeiden\n",
    "if 'spark' not in locals() or spark.sparkContext._jsc is None:\n",
    "    print(\"‚è≥ Starte Spark Session (das kann 5-10sek dauern)...\")\n",
    "\n",
    "    # Konfiguration f√ºr maximale lokale Performance\n",
    "    conf = SparkConf() \\\n",
    "        .setAppName(\"OpenSkyIngest\") \\\n",
    "        .setMaster(\"local[*]\") \\\n",
    "        .set(\"spark.sql.shuffle.partitions\", \"20\") # Optimierung f√ºr kleinere Datenmengen\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    print(f\"‚úÖ Spark Session gestartet. Version: {spark.version}\")\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è Spark l√§uft bereits (Version: {spark.version})\")\n",
    "\n",
    "# 2. DYNAMISCHE BENCHMARK EXECUTION\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Dynamische Szenarien basierend auf dem Config-Limit erstellen\n",
    "# (z.B. bei Limit 50 -> [5, 10, 25, 50])\n",
    "limit = PipelineConfig.FLIGHT_DATA_LIMIT\n",
    "scenarios = sorted(list(set([5, int(limit * 0.2), int(limit * 0.5), limit])))\n",
    "scenarios = [s for s in scenarios if s > 0]  # Filtert Nullwerte\n",
    "\n",
    "csv_file = PipelineConfig.METRICS_FILE.replace(\".json\", \"_history.csv\") # Speichern im Metrics Ordner\n",
    "\n",
    "print(f\"\\n--- Starte Benchmark f√ºr max. {limit} Dateien ---\")\n",
    "print(f\"Geplante Szenarien: {scenarios}\")\n",
    "print(f\"Ergebnisse werden gespeichert in: {csv_file}\")\n",
    "\n",
    "# Header f√ºr CSV vorbereiten\n",
    "with open(csv_file, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"files_count\", \"duration_sec\", \"avg_cpu\", \"avg_ram\", \"net_mb\", \"disk_mb\"])\n",
    "\n",
    "# Schleife √ºber alle Szenarien\n",
    "for n_files in scenarios:\n",
    "    print(f\"\\nüöÄ Testlauf mit {n_files} Dateien...\")\n",
    "\n",
    "    # Monitor starten\n",
    "    monitor = PerformanceMonitor()\n",
    "    monitor.start()\n",
    "\n",
    "    try:\n",
    "        # Aufruf der Ingest-Funktion\n",
    "        # Wir √ºbergeben die aktive 'spark' Session\n",
    "        ingest_flight_data_spark(PipelineConfig, spark, file_limit=n_files)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler bei {n_files} Files: {e}\")\n",
    "\n",
    "    # Monitor stoppen & Stats holen\n",
    "    monitor.stop()\n",
    "    stats = monitor.get_stats()\n",
    "\n",
    "    # Ergebnisse in CSV schreiben\n",
    "    with open(csv_file, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            n_files,\n",
    "            round(stats['duration'], 2),\n",
    "            round(stats['avg_cpu'], 1),\n",
    "            round(stats['avg_ram'], 1),\n",
    "            round(stats['total_net_mb'], 2),\n",
    "            round(stats['total_disk_mb'], 2)\n",
    "        ])\n",
    "\n",
    "    print(f\"   -> Dauer: {stats['duration']:.1f}s | Net: {stats['total_net_mb']:.0f}MB\")\n",
    "\n",
    "print(f\"\\n‚úÖ Benchmark abgeschlossen. Daten liegen bereit f√ºr Visualisierung.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de041c7a89ef9ff2",
   "metadata": {},
   "source": [
    "## 5. Visualisierung & Performance-Analyse\n",
    "\n",
    "Abschlie√üend visualisieren wir die w√§hrend des Benchmarks gesammelten Rohdaten. Dies dient als **Qualit√§tsnachweis** f√ºr die optimierte Pipeline.\n",
    "\n",
    "Das Skript generiert ein Dashboard mit vier Metriken:\n",
    "\n",
    "1.  **Laufzeit vs. Menge:** Hier pr√ºfen wir die Skalierbarkeit.\n",
    "    * *Erwartung:* Eine lineare Kurve.\n",
    "    * *Warnsignal:* Ein exponentieller Anstieg bei 50 Dateien deutet auf das identifizierte **S3-Rate-Limiting** hin.\n",
    "2.  **Hardware-Auslastung (CPU & RAM):**\n",
    "    * Hier beweisen wir den Erfolg des **Streaming-Ansatzes**.\n",
    "    * Die RAM-Kurve sollte flach bleiben (konstanter Speicherverbrauch), auch wenn wir mehr Dateien laden.\n",
    "3.  **Netzwerk-Traffic:**\n",
    "    * Zeigt das tats√§chliche Datenvolumen. Dies best√§tigt, dass wir keine unn√∂tigen Daten mehr laden (Padding-Fix).\n",
    "4.  **Effizienz (Sekunden pro Datei):**\n",
    "    * Diese Metrik zeigt, wie \"teuer\" eine einzelne Datei ist.\n",
    "    * Steigt dieser Wert bei hoher Last an, ist dies der definitive Beweis f√ºr Drosselung durch die externe API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd90e0f8718a9dc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T22:28:48.649214Z",
     "start_time": "2026-02-03T22:28:48.239732Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\">>> Start: Detaillierte System-Analyse\")\n",
    "\n",
    "# WICHTIG: Pfad dynamisch aus der Config holen (wie im Schritt davor)\n",
    "# Falls PipelineConfig nicht verf√ºgbar ist, Fallback auf lokalen Pfad\n",
    "try:\n",
    "    csv_file = PipelineConfig.METRICS_FILE.replace(\".json\", \"_history.csv\")\n",
    "except NameError:\n",
    "    csv_file = \"benchmark_history.csv\"\n",
    "\n",
    "if os.path.exists(csv_file):\n",
    "    # Daten laden und sortieren\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df.sort_values(\"files_count\")\n",
    "\n",
    "    # Dubletten entfernen (falls du den Benchmark mehrfach hast laufen lassen)\n",
    "    df = df.drop_duplicates(subset=['files_count'], keep='last')\n",
    "\n",
    "    # Setup: 2x2 Grid\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    fig.suptitle('Benchmark Ergebnisse: OpenSky Ingest', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # --- GRAPH 1: DAUER ---\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(df['files_count'], df['duration_sec'], marker='o', color='#2c3e50', linewidth=2, label='Dauer (s)')\n",
    "    for i, val in enumerate(df['duration_sec']):\n",
    "        ax1.annotate(f\"{val:.0f}s\", (df['files_count'].iloc[i], val), xytext=(0, 10), textcoords='offset points', ha='center')\n",
    "    ax1.set_title(\"1. Laufzeit (Sekunden)\")\n",
    "    ax1.set_xlabel(\"Anzahl Dateien\")\n",
    "    ax1.set_ylabel(\"Sekunden\")\n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # --- GRAPH 2: CPU & RAM ---\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(df['files_count'], df['avg_cpu'], marker='s', color='#e74c3c', label='√ò CPU (%)')\n",
    "    ax2.plot(df['files_count'], df['avg_ram'], marker='d', color='#27ae60', label='√ò RAM (%)')\n",
    "    ax2.set_title(\"2. Hardware Auslastung (Beweis f√ºr Streaming)\")\n",
    "    ax2.set_ylim(0, 100)\n",
    "    ax2.set_ylabel(\"Auslastung (%)\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # --- GRAPH 3: NETZWERK VOLUMEN ---\n",
    "    ax3 = axes[1, 0]\n",
    "    bars = ax3.bar(df['files_count'].astype(str), df['net_mb'], color='#3498db', alpha=0.8)\n",
    "    ax3.set_title(\"3. Netzwerk Traffic (MB)\")\n",
    "    ax3.set_ylabel(\"MB geladen\")\n",
    "    # Werte auf Balken\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height, f'{height:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # --- GRAPH 4: EFFIZIENZ (Sekunden pro Datei) ---\n",
    "    ax4 = axes[1, 1]\n",
    "    sec_per_file = df['duration_sec'] / df['files_count']\n",
    "    ax4.plot(df['files_count'], sec_per_file, marker='x', color='#9b59b6', linestyle='--', linewidth=2)\n",
    "    ax4.set_title(\"4. Effizienz (Sekunden pro Datei)\")\n",
    "    ax4.set_ylabel(\"Sekunden / Datei\")\n",
    "    ax4.set_xlabel(\"Anzahl Dateien\")\n",
    "    ax4.grid(True)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    print(\"--- ZUSAMMENFASSUNG ---\")\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå Keine Benchmark-Daten gefunden unter: {csv_file}\")\n",
    "    print(\"Bitte f√ºhren Sie Schritt 4 (Benchmark) zuerst aus.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
