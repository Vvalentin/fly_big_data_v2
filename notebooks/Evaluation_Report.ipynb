{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Advanced Data Engineering - Evaluation Report\\n\",\n",
    "    \"\\n\",\n",
    "    \"Dieses Notebook wertet die Logs der `spark_ingest_s3.py` Pipeline aus und beantwortet die theoretischen Fragen zur Architektur.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Pfad zur Log-Datei\\n\",\n",
    "    \"log_path = '../data/pipeline_metrics.json'\\n\",\n",
    "    \"\\n\",\n",
    "    \"if os.path.exists(log_path):\\n\",\n",
    "    \"    with open(log_path, 'r') as f:\\n\",\n",
    "    \"        data = json.load(f)\\n\",\n",
    "    \"    df_logs = pd.DataFrame(data)\\n\",\n",
    "    \"    # Zeitstempel lesbar machen\\n\",\n",
    "    \"    df_logs['time_str'] = pd.to_datetime(df_logs['iso_date']).dt.strftime('%H:%M:%S')\\n\",\n",
    "    \"    display(df_logs.tail(10))\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"Keine Logs gefunden. Bitte erst src/spark_ingest_s3.py ausführen!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Scaling Analysis\\n\",\n",
    "    \"**Q: What happens when the amount of data increases (1x, 10x, 100x)?**\\n\",\n",
    "    \"\\n\",\n",
    "    \"Um dies zu testen, wurde `SCALE_FACTOR` in der `.env` Datei variiert. Hier ist der Verlauf der Verarbeitungszeit:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if 'metric' in df_logs.columns:\\n\",\n",
    "    \"    # Wir filtern nach der Metrik 'Duration' (Verarbeitungszeit)\\n\",\n",
    "    \"    duration_data = df_logs[df_logs['metric'] == 'Duration']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if not duration_data.empty:\\n\",\n",
    "    \"        plt.figure(figsize=(10, 5))\\n\",\n",
    "    \"        sns.barplot(data=duration_data, x='iso_date', y='value', color='skyblue')\\n\",\n",
    "    \"        plt.title('Processing Time per Run (Scaling Test)')\\n\",\n",
    "    \"        plt.ylabel('Seconds')\\n\",\n",
    "    \"        plt.xlabel('Run Timestamp')\\n\",\n",
    "    \"        plt.xticks(rotation=45)\\n\",\n",
    "    \"        plt.show()\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"Noch keine 'Duration' Logs vorhanden.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Interpretation der Skalierung\\n\",\n",
    "    \"\\n\",\n",
    "    \"* **Horizontal Scaling:** Spark verteilt die Last über `mapPartitions`. Wenn wir mehr Worker (Nodes) hinzufügen, sinkt die Zeit fast linear, solange die Anzahl der Partitionen >= Anzahl der Kerne ist.\\n\",\n",
    "    \"* **IO-Bound vs. CPU-Bound:** \\n\",\n",
    "    \"  * Unsere Ingestion ist stark **Network I/O Bound**, da wir große `.tar` Dateien von S3 laden.\\n\",\n",
    "    \"  * Sie ist **CPU Bound** beim Entpacken (`tarfile`) und Parsing (UTF-8 Decode).\\n\",\n",
    "    \"* **Skew-Risiko:** Da `.tar`-Archive nicht splittbar sind, muss eine Datei immer von *einem* Core komplett verarbeitet werden. Wenn eine Datei 10GB groß ist und die anderen nur 100MB, warten alle Worker auf den einen (\\\"Straggler Problem\\\").\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Fault Tolerance\\n\",\n",
    "    \"**Q: How does the system behave under failures?**\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. **Worker Failure (OOM):** Wenn ein Worker beim Entpacken einer riesigen Datei abstürzt (Out Of Memory), erkennt der Spark Driver den Verlust der Partition. Dank **RDD Lineage** weiß der Driver, wie er die Daten neu beschaffen kann: Er startet den Task auf einem anderen Node neu.\\n\",\n",
    "    \"2. **Network Interruption:** Da wir `boto3` nutzen, würde eine Netzwerkunterbrechung eine Exception werfen. Im Code (`spark_ingest_s3.py`) fangen wir diese (`try-except`) und geben einen Fehler-String zurück. Das verhindert, dass der ganze Spark-Job abbricht (Graceful Degradation).\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Architektur-Diagramm (Text):**\\n\",\n",
    "    \"`S3 (Source)` -> `Worker (Download & Untar im RAM)` -> `RDD Partition` -> `DataFrame` -> `Parquet (Disk)`\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.5\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Pfad zur Log-Datei\n",
    "log_path = '../data/pipeline_metrics.json'\n",
    "\n",
    "if os.path.exists(log_path):\n",
    "    with open(log_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    df_logs = pd.DataFrame(data)\n",
    "    # Zeitstempel lesbar machen\n",
    "    df_logs['time_str'] = pd.to_datetime(df_logs['iso_date']).dt.strftime('%H:%M:%S')\n",
    "    display(df_logs.tail(10))\n",
    "else:\n",
    "    print(\"Keine Logs gefunden. Bitte erst src/spark_ingest_s3.py ausführen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scaling Analysis\n",
    "**Q: What happens when the amount of data increases (1x, 10x, 100x)?**\n",
    "\n",
    "Um dies zu testen, wurde `SCALE_FACTOR` in der `.env` Datei variiert. Hier ist der Verlauf der Verarbeitungszeit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'metric' in df_logs.columns:\n",
    "    # Wir filtern nach der Metrik 'Duration' (Verarbeitungszeit)\n",
    "    duration_data = df_logs[df_logs['metric'] == 'Duration']\n",
    "    \n",
    "    if not duration_data.empty:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.barplot(data=duration_data, x='iso_date', y='value', color='skyblue')\n",
    "        plt.title('Processing Time per Run (Scaling Test)')\n",
    "        plt.ylabel('Seconds')\n",
    "        plt.xlabel('Run Timestamp')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Noch keine 'Duration' Logs vorhanden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation der Skalierung\n",
    "\n",
    "* **Horizontal Scaling:** Spark verteilt die Last über `mapPartitions`. Wenn wir mehr Worker (Nodes) hinzufügen, sinkt die Zeit fast linear, solange die Anzahl der Partitionen >= Anzahl der Kerne ist.\n",
    "* **IO-Bound vs. CPU-Bound:** \n",
    "  * Unsere Ingestion ist stark **Network I/O Bound**, da wir große `.tar` Dateien von S3 laden.\n",
    "  * Sie ist **CPU Bound** beim Entpacken (`tarfile`) und Parsing (UTF-8 Decode).\n",
    "* **Skew-Risiko:** Da `.tar`-Archive nicht splittbar sind, muss eine Datei immer von *einem* Core komplett verarbeitet werden. Wenn eine Datei 10GB groß ist und die anderen nur 100MB, warten alle Worker auf den einen (\"Straggler Problem\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fault Tolerance\n",
    "**Q: How does the system behave under failures?**\n",
    "\n",
    "1. **Worker Failure (OOM):** Wenn ein Worker beim Entpacken einer riesigen Datei abstürzt (Out Of Memory), erkennt der Spark Driver den Verlust der Partition. Dank **RDD Lineage** weiß der Driver, wie er die Daten neu beschaffen kann: Er startet den Task auf einem anderen Node neu.\n",
    "2. **Network Interruption:** Da wir `boto3` nutzen, würde eine Netzwerkunterbrechung eine Exception werfen. Im Code (`spark_ingest_s3.py`) fangen wir diese (`try-except`) und geben einen Fehler-String zurück. Das verhindert, dass der ganze Spark-Job abbricht (Graceful Degradation).\n",
    "\n",
    "**Architektur-Diagramm (Text):**\n",
    "`S3 (Source)` -> `Worker (Download & Untar im RAM)` -> `RDD Partition` -> `DataFrame` -> `Parquet (Disk)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
