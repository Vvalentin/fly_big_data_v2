{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-03T19:47:37.025957Z",
     "start_time": "2026-02-03T19:47:34.327185Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import logging\n",
    "import threading\n",
    "import psutil\n",
    "import boto3\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# PySpark & Boto3 Config\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config as BotoConfig\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType\n",
    "\n",
    "# --- WINDOWS UMWGEBUNGS-KONFIGURATION ---\n",
    "# Pfade explizit setzen, um WinError 2 zu vermeiden\n",
    "JAVA_HOME_RAW = r\"C:\\ProgramData\\jdk-11.0.29.7-hotspot\"\n",
    "HADOOP_HOME_RAW = r\"C:\\hadoop\"\n",
    "\n",
    "os.environ['JAVA_HOME'] = JAVA_HOME_RAW\n",
    "os.environ['HADOOP_HOME'] = HADOOP_HOME_RAW\n",
    "\n",
    "# Pfade f√ºr Binaries bauen\n",
    "java_bin = os.path.join(JAVA_HOME_RAW, 'bin')\n",
    "hadoop_bin = os.path.join(HADOOP_HOME_RAW, 'bin')\n",
    "\n",
    "# PATH Variable erweitern\n",
    "current_path = os.environ.get('PATH', '')\n",
    "if java_bin not in current_path:\n",
    "    os.environ['PATH'] = java_bin + \";\" + current_path\n",
    "if hadoop_bin not in current_path:\n",
    "    os.environ['PATH'] = hadoop_bin + \";\" + os.environ['PATH']\n",
    "\n",
    "# Spark Home automatisch finden\n",
    "import pyspark\n",
    "spark_home_candidate = os.path.dirname(pyspark.__file__)\n",
    "if os.path.exists(os.path.join(spark_home_candidate, \"bin\")):\n",
    "    os.environ['SPARK_HOME'] = spark_home_candidate\n",
    "    os.environ['PATH'] = os.path.join(spark_home_candidate, \"bin\") + \";\" + os.environ['PATH']\n",
    "\n",
    "# Logging Init\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"Pipeline\")\n",
    "\n",
    "print(\"‚úÖ Umgebungsvariablen gesetzt. Imports vollst√§ndig.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Umgebungsvariablen gesetzt. Imports vollst√§ndig.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T19:47:37.048176Z",
     "start_time": "2026-02-03T19:47:37.037133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PipelineConfig:\n",
    "    # --- STEUERUNG DATENMENGE ---\n",
    "    # Wie viele Dateien sollen im Standard-Durchlauf geladen werden?\n",
    "    FLIGHT_DATA_LIMIT = 25\n",
    "\n",
    "    # S3 Settings (OpenSky)\n",
    "    S3_BUCKET = \"data-samples\"\n",
    "    S3_ENDPOINT = \"https://s3.opensky-network.org\"\n",
    "\n",
    "    # Dateinamen Input\n",
    "    AIRCRAFT_CSV_NAME = \"aircraft-database-complete-2025-08.csv\"\n",
    "\n",
    "    # --- OUTPUT PFADE ---\n",
    "    BASE_OUTPUT_DIR = os.path.join(os.getcwd(), \"project_data_output\")\n",
    "\n",
    "    # Unterordner\n",
    "    RAW_DIR = os.path.join(BASE_OUTPUT_DIR, \"raw\")\n",
    "    PROCESSED_DIR = os.path.join(BASE_OUTPUT_DIR, \"processed\")\n",
    "    METRICS_DIR = os.path.join(BASE_OUTPUT_DIR, \"metrics\")\n",
    "\n",
    "    # Dateinamen Output\n",
    "    AIRCRAFT_PARQUET_NAME = \"aircraft_database.parquet\"\n",
    "    FLIGHT_DATA_OUTPUT_NAME = \"flight_data_batch.parquet\"\n",
    "    METRICS_FILE = os.path.join(METRICS_DIR, \"pipeline_metrics.json\")\n",
    "    BENCHMARK_CSV = \"benchmark_history.csv\" # Liegt direkt im Notebook-Ordner f√ºr einfache Analyse\n",
    "\n",
    "def init_directories():\n",
    "    \"\"\"Erstellt die notwendige Ordnerstruktur\"\"\"\n",
    "    for path in [PipelineConfig.RAW_DIR, PipelineConfig.PROCESSED_DIR, PipelineConfig.METRICS_DIR]:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "    print(f\"‚úÖ Verzeichnisse initialisiert unter: {PipelineConfig.BASE_OUTPUT_DIR}\")\n",
    "\n",
    "init_directories()"
   ],
   "id": "9ea97f6fa0461529",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Verzeichnisse initialisiert unter: C:\\Users\\valim\\Master\\AdvancedDataEngineering\\FlyBigData_v2\\notebooks\\Use Cases\\project_data_output\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T19:47:37.090255Z",
     "start_time": "2026-02-03T19:47:37.074216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- A. PERFORMANCE MONITOR ---\n",
    "class PerformanceMonitor:\n",
    "    def __init__(self):\n",
    "        self.running = False\n",
    "        self.cpu_usage = []\n",
    "        self.ram_usage = []\n",
    "        self.net_sent = []\n",
    "        self.net_recv = []\n",
    "        self.start_time = 0\n",
    "        self._thread = None\n",
    "\n",
    "    def _monitor_loop(self):\n",
    "        net_io_start = psutil.net_io_counters()\n",
    "        start_bytes_recv = net_io_start.bytes_recv\n",
    "\n",
    "        while self.running:\n",
    "            self.cpu_usage.append(psutil.cpu_percent(interval=None))\n",
    "            self.ram_usage.append(psutil.virtual_memory().percent)\n",
    "\n",
    "            net_now = psutil.net_io_counters()\n",
    "            self.net_recv.append((net_now.bytes_recv - start_bytes_recv) / (1024 * 1024))\n",
    "            time.sleep(1)\n",
    "\n",
    "    def start(self):\n",
    "        self.running = True\n",
    "        self.start_time = time.time()\n",
    "        self.cpu_usage, self.ram_usage, self.net_recv = [], [], []\n",
    "        psutil.cpu_percent(interval=None) # Init call\n",
    "        self._thread = threading.Thread(target=self._monitor_loop)\n",
    "        self._thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        if self._thread:\n",
    "            self._thread.join()\n",
    "\n",
    "    def get_stats(self):\n",
    "        if not self.cpu_usage: return {}\n",
    "        return {\n",
    "            \"duration\": time.time() - self.start_time,\n",
    "            \"avg_cpu\": np.mean(self.cpu_usage),\n",
    "            \"avg_ram\": np.mean(self.ram_usage),\n",
    "            \"total_net_mb\": self.net_recv[-1] if self.net_recv else 0\n",
    "        }\n",
    "\n",
    "# --- B. AIRCRAFT INGEST (PANDAS) ---\n",
    "def ingest_aircraft_database(config):\n",
    "    logger.info(\"--- Start: Aircraft Database Ingest ---\")\n",
    "    s3 = boto3.client('s3', endpoint_url=config.S3_ENDPOINT, config=BotoConfig(signature_version=UNSIGNED))\n",
    "\n",
    "    # Suchen und Laden\n",
    "    key = f\"metadata/{config.AIRCRAFT_CSV_NAME}\"\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=config.S3_BUCKET, Key=key)\n",
    "        df = pd.read_csv(obj['Body'], dtype=str, quotechar=\"'\", on_bad_lines='skip')\n",
    "\n",
    "        # Transformation\n",
    "        cols = ['icao24', 'manufacturerName', 'model', 'typecode', 'operator']\n",
    "        df = df[[c for c in cols if c in df.columns]].drop_duplicates(subset=['icao24'])\n",
    "\n",
    "        out_path = os.path.join(config.PROCESSED_DIR, config.AIRCRAFT_PARQUET_NAME)\n",
    "        df.to_parquet(out_path, index=False)\n",
    "        logger.info(f\"‚úÖ Aircraft Parquet gespeichert: {out_path}\")\n",
    "        return out_path\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler Aircraft Ingest: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- C. FLIGHT DATA INGEST (SPARK) ---\n",
    "def ingest_flight_data_spark(config, spark_session, file_limit=None):\n",
    "    logger.info(\"--- Start: Flight Data Spark Ingest ---\")\n",
    "    s3 = boto3.client('s3', endpoint_url=config.S3_ENDPOINT, config=BotoConfig(signature_version=UNSIGNED))\n",
    "\n",
    "    # 1. Dateien auflisten (csv.gz bevorzugt)\n",
    "    limit = file_limit if file_limit else config.FLIGHT_DATA_LIMIT\n",
    "    all_files = []\n",
    "\n",
    "    try:\n",
    "        paginator = s3.get_paginator('list_objects_v2')\n",
    "        # Wir suchen in flights/ nach den komprimierten CSVs\n",
    "        for page in paginator.paginate(Bucket=config.S3_BUCKET, Prefix=\"flights/\"):\n",
    "            if 'Contents' not in page: continue\n",
    "            for obj in page['Contents']:\n",
    "                if obj['Key'].endswith('.csv.gz'):\n",
    "                    all_files.append(obj['Key'])\n",
    "                if len(all_files) >= limit: break\n",
    "            if len(all_files) >= limit: break\n",
    "    except Exception as e:\n",
    "        logger.error(f\"S3 Listing Fehler: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Falls nicht genug Dateien da sind, Liste duplizieren f√ºr Stresstest\n",
    "    if all_files and len(all_files) < limit:\n",
    "        while len(all_files) < limit:\n",
    "            all_files.extend(all_files[:limit - len(all_files)])\n",
    "\n",
    "    file_list = all_files[:limit]\n",
    "    logger.info(f\"Starte Verarbeitung f√ºr {len(file_list)} Dateien...\")\n",
    "\n",
    "    # 2. Messung & Spark Job\n",
    "    monitor = PerformanceMonitor()\n",
    "    monitor.start()\n",
    "    t_start = time.time()\n",
    "\n",
    "    # Worker Funktion f√ºr Spark (liest S3 Stream)\n",
    "    def worker(iter):\n",
    "        s3_worker = boto3.client('s3', endpoint_url=\"https://s3.opensky-network.org\", config=BotoConfig(signature_version=UNSIGNED))\n",
    "        for k in iter:\n",
    "            try:\n",
    "                o = s3_worker.get_object(Bucket=\"data-samples\", Key=k)\n",
    "                with gzip.open(BytesIO(o['Body'].read()), 'rt', encoding='utf-8') as f:\n",
    "                    for _ in f: pass # Dummy Read f√ºr IO Load\n",
    "            except: pass\n",
    "            yield (k,)\n",
    "\n",
    "    # Ausf√ºhrung\n",
    "    rdd = spark_session.sparkContext.parallelize(file_list, numSlices=max(2, len(file_list)))\n",
    "    count = rdd.mapPartitions(worker).count()\n",
    "\n",
    "    duration = time.time() - t_start\n",
    "    monitor.stop()\n",
    "    stats = monitor.get_stats()\n",
    "\n",
    "    # 3. CSV Log schreiben\n",
    "    file_exists = os.path.isfile(config.BENCHMARK_CSV)\n",
    "    with open(config.BENCHMARK_CSV, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists:\n",
    "            writer.writerow([\"timestamp\", \"files_count\", \"duration_sec\", \"avg_cpu\", \"avg_ram\", \"net_mb\"])\n",
    "\n",
    "        writer.writerow([\n",
    "            datetime.now().isoformat(),\n",
    "            len(file_list),\n",
    "            round(duration, 2),\n",
    "            round(stats.get('avg_cpu', 0), 1),\n",
    "            round(stats.get('avg_ram', 0), 1),\n",
    "            round(stats.get('total_net_mb', 0), 2)\n",
    "        ])\n",
    "\n",
    "    logger.info(f\"‚úÖ Fertig: {len(file_list)} Files in {duration:.2f}s\")\n",
    "    return count"
   ],
   "id": "4028ecf49fd03f59",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T19:53:39.030647Z",
     "start_time": "2026-02-03T19:47:37.097098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Spark Session Starten\n",
    "print(\"üöÄ Starte Spark Session...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OpenSkyPipeline\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "\n",
    "# 2. Aircraft Stammdaten (Einmalig)\n",
    "print(\"\\n--- Schritt 1: Aircraft Stammdaten ---\")\n",
    "ingest_aircraft_database(PipelineConfig)\n",
    "\n",
    "# 3. Flugdaten (Benchmark Loop f√ºr Analyse)\n",
    "print(\"\\n--- Schritt 2: Flugdaten Benchmark ---\")\n",
    "# Wir simulieren steigende Last (5, 10, 20 Dateien), um sch√∂ne Graphen zu bekommen\n",
    "scenarios = [5, 10, 20]\n",
    "\n",
    "for n_files in scenarios:\n",
    "    print(f\"\\n>> Starte Szenario mit {n_files} Dateien...\")\n",
    "    ingest_flight_data_spark(PipelineConfig, spark, file_limit=n_files)\n",
    "\n",
    "print(\"\\nüèÜ Pipeline und Benchmark erfolgreich abgeschlossen.\")"
   ],
   "id": "685186f26211db82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starte Spark Session...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 20:47:43,531 - INFO - --- Start: Aircraft Database Ingest ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.0\n",
      "\n",
      "--- Schritt 1: Aircraft Stammdaten ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 20:53:25,856 - INFO - ‚úÖ Aircraft Parquet gespeichert: C:\\Users\\valim\\Master\\AdvancedDataEngineering\\FlyBigData_v2\\notebooks\\Use Cases\\project_data_output\\processed\\aircraft_database.parquet\n",
      "2026-02-03 20:53:25,876 - INFO - --- Start: Flight Data Spark Ingest ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Schritt 2: Flugdaten Benchmark ---\n",
      "\n",
      ">> Starte Szenario mit 5 Dateien...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 20:53:26,427 - INFO - Starte Verarbeitung f√ºr 5 Dateien...\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2) (Valis executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 23\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m n_files \u001B[38;5;129;01min\u001B[39;00m scenarios:\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m>> Starte Szenario mit \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mn_files\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Dateien...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 23\u001B[0m     \u001B[43mingest_flight_data_spark\u001B[49m\u001B[43m(\u001B[49m\u001B[43mPipelineConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspark\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfile_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_files\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124müèÜ Pipeline und Benchmark erfolgreich abgeschlossen.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[3], line 118\u001B[0m, in \u001B[0;36mingest_flight_data_spark\u001B[1;34m(config, spark_session, file_limit)\u001B[0m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;66;03m# Ausf√ºhrung\u001B[39;00m\n\u001B[0;32m    117\u001B[0m rdd \u001B[38;5;241m=\u001B[39m spark_session\u001B[38;5;241m.\u001B[39msparkContext\u001B[38;5;241m.\u001B[39mparallelize(file_list, numSlices\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mmax\u001B[39m(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;28mlen\u001B[39m(file_list)))\n\u001B[1;32m--> 118\u001B[0m count \u001B[38;5;241m=\u001B[39m \u001B[43mrdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmapPartitions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mworker\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcount\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    120\u001B[0m duration \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m t_start\n\u001B[0;32m    121\u001B[0m monitor\u001B[38;5;241m.\u001B[39mstop()\n",
      "File \u001B[1;32m~\\Master\\AdvancedDataEngineering\\FlyBigData_v2\\.venv1\\Lib\\site-packages\\pyspark\\rdd.py:2316\u001B[0m, in \u001B[0;36mRDD.count\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   2295\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcount\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[0;32m   2296\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2297\u001B[0m \u001B[38;5;124;03m    Return the number of elements in this RDD.\u001B[39;00m\n\u001B[0;32m   2298\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2314\u001B[0m \u001B[38;5;124;03m    3\u001B[39;00m\n\u001B[0;32m   2315\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 2316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmapPartitions\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43msum\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Master\\AdvancedDataEngineering\\FlyBigData_v2\\.venv1\\Lib\\site-packages\\pyspark\\rdd.py:2291\u001B[0m, in \u001B[0;36mRDD.sum\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   2270\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21msum\u001B[39m(\u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[NumberOrArray]\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumberOrArray\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m   2271\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2272\u001B[0m \u001B[38;5;124;03m    Add up the elements in this RDD.\u001B[39;00m\n\u001B[0;32m   2273\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2289\u001B[0m \u001B[38;5;124;03m    6.0\u001B[39;00m\n\u001B[0;32m   2290\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 2291\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmapPartitions\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43msum\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfold\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[return-value]\u001B[39;49;00m\n\u001B[0;32m   2292\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\n\u001B[0;32m   2293\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Master\\AdvancedDataEngineering\\FlyBigData_v2\\.venv1\\Lib\\site-packages\\pyspark\\rdd.py:2044\u001B[0m, in \u001B[0;36mRDD.fold\u001B[1;34m(self, zeroValue, op)\u001B[0m\n\u001B[0;32m   2039\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m acc\n\u001B[0;32m   2041\u001B[0m \u001B[38;5;66;03m# collecting result of mapPartitions here ensures that the copy of\u001B[39;00m\n\u001B[0;32m   2042\u001B[0m \u001B[38;5;66;03m# zeroValue provided to each partition is unique from the one provided\u001B[39;00m\n\u001B[0;32m   2043\u001B[0m \u001B[38;5;66;03m# to the final reduce call\u001B[39;00m\n\u001B[1;32m-> 2044\u001B[0m vals \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmapPartitions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2045\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001B[1;32m~\\Master\\AdvancedDataEngineering\\FlyBigData_v2\\.venv1\\Lib\\site-packages\\pyspark\\rdd.py:1833\u001B[0m, in \u001B[0;36mRDD.collect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1831\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SCCallSiteSync(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext):\n\u001B[0;32m   1832\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1833\u001B[0m     sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonRDD\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectAndServe\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jrdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrdd\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1834\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jrdd_deserializer))\n",
      "File \u001B[1;32m~\\Master\\AdvancedDataEngineering\\FlyBigData_v2\\.venv1\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32m~\\Master\\AdvancedDataEngineering\\FlyBigData_v2\\.venv1\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    180\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    181\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[1;32m~\\Master\\AdvancedDataEngineering\\FlyBigData_v2\\.venv1\\Lib\\site-packages\\py4j\\protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[1;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2) (Valis executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- PERFORMANCE VISUALISIERUNG ---\n",
    "csv_file = PipelineConfig.BENCHMARK_CSV\n",
    "\n",
    "if not os.path.exists(csv_file):\n",
    "    print(\"‚ùå Keine Benchmark-Daten gefunden. Bitte Pipeline Block ausf√ºhren!\")\n",
    "else:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # Sortieren nach Files Count f√ºr sauberen Graphen\n",
    "    df = df.sort_values(by=\"files_count\")\n",
    "\n",
    "    plt.style.use('ggplot')\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Plot 1: Skalierbarkeit (Zeit vs Anzahl Files)\n",
    "    ax1.plot(df['files_count'], df['duration_sec'], marker='o', linewidth=2, color='#2980b9', label='Messwerte')\n",
    "\n",
    "    # Trendlinie\n",
    "    if len(df) > 1:\n",
    "        z = np.polyfit(df['files_count'], df['duration_sec'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax1.plot(df['files_count'], p(df['files_count']), \"r--\", alpha=0.6, label='Linearer Trend')\n",
    "\n",
    "    ax1.set_title(\"Skalierbarkeit: Laufzeit\")\n",
    "    ax1.set_xlabel(\"Anzahl Dateien\")\n",
    "    ax1.set_ylabel(\"Dauer (Sekunden)\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot 2: Ressourcen\n",
    "    ax2.set_title(\"Ressourcen-Auslastung\")\n",
    "    ax2.set_xlabel(\"Anzahl Dateien\")\n",
    "    ax2.set_ylabel(\"CPU / RAM (%)\")\n",
    "\n",
    "    l1 = ax2.plot(df['files_count'], df['avg_cpu'], marker='s', color='#e74c3c', label='√ò CPU')\n",
    "    l2 = ax2.plot(df['files_count'], df['avg_ram'], marker='d', color='#27ae60', label='√ò RAM')\n",
    "\n",
    "    # Zweite Y-Achse f√ºr Netzwerk\n",
    "    ax2_net = ax2.twinx()\n",
    "    l3 = ax2_net.plot(df['files_count'], df['net_mb'], marker='^', linestyle='--', color='#8e44ad', label='Netzwerk (MB)')\n",
    "    ax2_net.set_ylabel(\"Datenvolumen (MB)\")\n",
    "\n",
    "    # Legende\n",
    "    lns = l1 + l2 + l3\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax2.legend(lns, labs, loc='center right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Analyse:\")\n",
    "    print(f\"Datens√§tze analysiert: {len(df)}\")\n",
    "    print(f\"Maximaler Traffic: {df['net_mb'].max():.2f} MB\")"
   ],
   "id": "d9cf7708d3ce61a2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
